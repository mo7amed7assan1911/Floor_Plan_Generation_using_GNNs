{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from encoding_boundaries import GraphEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 9 nodes and 9 edges\n"
     ]
    }
   ],
   "source": [
    "url = r\"D:\\Grad\\Planify_Dataset\\Graph\\graphs\\boundaries.pkl\"\n",
    "\n",
    "with open(url, 'rb') as f:\n",
    "    boundaries = pickle.load(f)\n",
    "    \n",
    "G = boundaries[1911]\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80787/80787 [00:48<00:00, 1655.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 26], x=[13, 3], edge_attr=[26, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting networkx boundary graphs to pytorchGeo graphs\n",
    "Boundaries_pyTorch = []\n",
    "for b in tqdm(boundaries):\n",
    "    b_new = from_networkx(b, group_node_attrs=['type', 'centroid'], group_edge_attrs=['distance'])\n",
    "    \n",
    "    Boundaries_pyTorch.append(b_new)\n",
    "\n",
    "Boundaries_pyTorch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80787/80787 [00:12<00:00, 6324.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Normalization for the centroid coordinates\n",
    "for b in tqdm(Boundaries_pyTorch, total=len(Boundaries_pyTorch)):\n",
    "    x = b.x # The feature matrix\n",
    "    for i in [1, 2]:\n",
    "        mean = torch.mean(x[:, i])\n",
    "        std  = torch.std(x[:, i])\n",
    "        \n",
    "        normalized_column = (x[:, i] - mean) / std\n",
    "        b.x[:, i] = normalized_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "def save_checkpoint(model, optimizer, epoch):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'Best_model.pth')\n",
    "    # Saving model each 15 epochs\n",
    "#     if epoch % 15 == 0:\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 64629, val: 16148, test: 10\n"
     ]
    }
   ],
   "source": [
    "edge = int(len(Boundaries_pyTorch) * 0.8)\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = Boundaries_pyTorch[:edge]\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = Boundaries_pyTorch[edge:-10]\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = Boundaries_pyTorch[-10:]\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print(f\"train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = Boundaries_pyTorch[0].x.shape[1]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = GraphEncoder(num_of_features, 64).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.950)\n",
    "type_criterion = nn.BCEWithLogitsLoss()\n",
    "centroid_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, type_criterion, centroid_criterion, device):\n",
    "    model.train()\n",
    "    total_loss1 = 0.0\n",
    "    total_loss2 = 0.0\n",
    "    for data in tqdm(train_loader, total=len(train_loader)):\n",
    "        features   = data.x.to(device).float()\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        \n",
    "        type_target     = features[:, 0].unsqueeze(1)\n",
    "        centroid_target = features[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        type_pred, centroid_pred = model(features, edge_index)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss1 = type_criterion(type_pred, type_target)\n",
    "        loss2 = centroid_criterion(centroid_pred, centroid_target)\n",
    "        loss  = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss1 += loss1.item()\n",
    "        total_loss2 += loss2.item()\n",
    "\n",
    "    avg_loss1 = total_loss1 / len(train_loader)\n",
    "    avg_loss2 = total_loss2 / len(train_loader)\n",
    "    \n",
    "    return avg_loss1, avg_loss2\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            features, adj = data.x, data.edge_index\n",
    "            features = features.to(device).float()\n",
    "            adj = adj.to(device)\n",
    "\n",
    "            reconstructed_features = model(features, adj)\n",
    "            loss = criterion(reconstructed_features, features)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 5\n",
    "patience = 20 # Number of epochs to wait if validation loss doesn't improve\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2020/2020 [00:23<00:00, 85.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss1: 0.6941, Train Loss2: 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2020/2020 [00:23<00:00, 85.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss1: 0.6932, Train Loss2: 0.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2020/2020 [00:24<00:00, 83.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss1: 0.6932, Train Loss2: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2020/2020 [00:24<00:00, 83.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss1: 0.6931, Train Loss2: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2020/2020 [00:24<00:00, 83.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss1: 0.6931, Train Loss2: 0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    train_loss = train(model, train_loader, optimizer, type_criterion, centroid_criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # # Evaluation loop\n",
    "    # print('Validating ...')\n",
    "    # val_loss = validate(model, val_loader, criterion, device)\n",
    "    # val_losses.append(val_loss)\n",
    "    \n",
    "    # Printing and monitoring\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss1: {train_loss[0]:.4f}, Train Loss2: {train_loss[1]:.4f}')\n",
    "    \n",
    "    \n",
    "    # # Early stopping\n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     best_model = deepcopy(model)\n",
    "    #     save_checkpoint(best_model, optimizer, epoch)\n",
    "    #     counter = 0\n",
    "    # else:\n",
    "    #     print('Model not saved!')\n",
    "    #     counter += 1\n",
    "    #     if counter >= patience:\n",
    "    #         print(f'Validation loss did not improve for {patience} epochs. Stopping early.')\n",
    "    #         break\n",
    "    #     if counter in [3, 5, 7] :\n",
    "    #         scheduler.step()\n",
    "    #         print('Learning rate decreased!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, adj = test_dataset[0].x, test_dataset[0].edge_index    \n",
    "features = features.to(device).float()\n",
    "adj = adj.to(device)\n",
    "type_pred, centroid_pred  = model(features, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[7.7189e-05],\n",
       "         [1.3967e-05],\n",
       "         [1.2893e-07],\n",
       "         [1.9446e-14],\n",
       "         [1.0663e-05]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " tensor([[-0.4066, -0.3699],\n",
       "         [-0.8329,  0.2884],\n",
       "         [ 0.4597,  0.7337],\n",
       "         [ 1.0983, -0.6929],\n",
       "         [-0.7653, -0.0688]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " None,\n",
       " tensor([[ 0.0000, -0.7212, -0.7753],\n",
       "         [ 0.0000, -0.7212,  1.0937],\n",
       "         [ 0.0000,  1.0954,  1.0937],\n",
       "         [ 0.0000,  1.0954, -0.7753],\n",
       "         [ 1.0000, -0.7483, -0.6368]], device='cuda:0'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_pred, centroid_pred , print(), features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
